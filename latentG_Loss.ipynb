{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from scipy.stats import multivariate_normal\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from torch import optim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import plotly.figure_factory as ff\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import load_npz\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from TextDataset import TextDataset\n",
    "from DualArchitecture import DualTextCNN\n",
    "from TextCNN import TextCNN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def proba_comb_features(gmm, comb_pred_vectors):\n",
    "    if comb_pred_vectors is None :\n",
    "        return None\n",
    "    predictions_gmm = gmm.predict(comb_pred_vectors)\n",
    "    pred_probs_gmm = gmm.predict_proba(comb_pred_vectors)\n",
    "\n",
    "    component = predictions_gmm[0]\n",
    "    mean = gmm.means_[component]\n",
    "    covariance = gmm.covariances_[component]\n",
    "    \n",
    "    pdf = multivariate_normal.pdf(comb_pred_vectors, mean=mean, cov=covariance)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "MODEL_SAVE_PATH = \"C:/Users/Korhan/Desktop/workspace/vsCodeWorkspace/Python_Workspace/mental_health_sentiment_analysis/dualtextcnnModel.pt\"\n",
    "BEST_MODEL_SAVE_PATH = \"C:/Users/Korhan/Desktop/workspace/vsCodeWorkspace/Python_Workspace/mental_health_sentiment_analysis/best_dualtextcnnModel.pt\"\n",
    "PRETRAINED_MODEL_PATH = \"C:/Users/Korhan/Desktop/workspace/vsCodeWorkspace/Python_Workspace/mental_health_sentiment_analysis/best_newdualtextcnnModel.pt\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "\n",
    "X_train_embeddings = np.load(\"X_train_embeddings.npy\")\n",
    "y_train_encoded = np.load(\"y_train_encoded.npy\")\n",
    "X_test_embeddings = np.load(\"X_test_embeddings.npy\")\n",
    "y_test_encoded = np.load(\"y_test_encoded.npy\")\n",
    "\n",
    "X_train_embeddings = np.array(X_train_embeddings)\n",
    "X_test_embeddings = np.array(X_test_embeddings)\n",
    "\n",
    "y_train_encoded = np.array(y_train_encoded)\n",
    "y_test_encoded = np.array(y_test_encoded)\n",
    "\n",
    "y_train_encoded = np.reshape(y_train_encoded, (-1,))\n",
    "y_test_encoded = np.reshape(y_test_encoded, (-1,))\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(X_train_embeddings, y_train_encoded)\n",
    "test_dataset = TextDataset(X_test_embeddings, y_test_encoded)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"Data is ready.\")\n",
    "\n",
    "teacher_model = DualTextCNN(input_dim=300, num_classes=7, latent_dim=32).to(DEVICE)\n",
    "teacher_model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "print(\"Teacher model has been loaded succesfully.\")\n",
    "\n",
    "teacher_model.eval()\n",
    "teacher_latent_embeddings = []\n",
    "teacher_pred_logits = []\n",
    "with torch.no_grad():\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(DEVICE).unsqueeze(1), y.to(DEVICE).squeeze(1)\n",
    "        y_pred, rec_pred, teacher_latent_embedding_batch = teacher_model(X)\n",
    "        teacher_pred_logits.append(y_pred)\n",
    "        teacher_latent_embeddings.append(teacher_latent_embedding_batch)\n",
    "teacher_latent_embeddings = torch.cat(teacher_latent_embeddings, dim=0)\n",
    "teacher_pred_logits = torch.cat(teacher_pred_logits, dim=0)\n",
    "teacher_combined_features = torch.cat( (teacher_latent_embeddings, teacher_pred_logits), dim=-1)\n",
    "teacher_combined_features = teacher_combined_features.detach().cpu().numpy()\n",
    "print(\"Teacher's Combined Features are ready.\")\n",
    "\n",
    "\n",
    "print(\"Fitting a Gaussian Mixture Model to Combined Features...\")\n",
    "param_grid = {\n",
    "'n_components': [7],\n",
    "'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n",
    "'max_iter': [100, 200, 500],\n",
    "}\n",
    "\n",
    "gmm = GaussianMixture()\n",
    "grid_search = GridSearchCV(gmm, param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(teacher_combined_features)\n",
    "\n",
    "best_gmm_model = grid_search.best_estimator_\n",
    "print(\"GMM fitted. Here are the results : \")\n",
    "print(\"Best Model: \", best_gmm_model)\n",
    "print(\"Gaussian Means Shape:\", best_gmm_model.means_.shape)\n",
    "print(\"Covariances Shape:\", best_gmm_model.covariances_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_limit = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting to the training of student model...\")\n",
    "student_model = DualTextCNN(input_dim=300, num_classes=7, latent_dim=32).to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9998)\n",
    "\n",
    "best_loss = 100\n",
    "best_model = deepcopy(student_model.state_dict())\n",
    "best_model_results = {\"train_loss\" : 100.0,\n",
    "                        \"test_loss\" : 100.0}\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    student_model.train()\n",
    "    train_running_loss = 0\n",
    "    for idx, (X,y) in enumerate(tqdm(train_dataloader)):\n",
    "        X = X.to(DEVICE).unsqueeze(1)\n",
    "        y = y.to(DEVICE).squeeze(1)\n",
    "        class_pred, reconstructed_pred, student_latent_vector_batch = student_model(X)\n",
    "\n",
    "        classification_loss = criterion1(class_pred, y)\n",
    "        reconstruction_loss = criterion2(reconstructed_pred, X)\n",
    "\n",
    "        comb_pred_vectors = torch.cat((student_latent_vector_batch , class_pred), dim=-1)\n",
    "        batch_data = teacher_pred_logits[idx* BATCH_SIZE : idx*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "        euclid_dist = torch.sqrt(torch.sum((class_pred - batch_data) ** 2, dim=1))\n",
    "        mean_distance = euclid_dist.mean()\n",
    "        std_distance = euclid_dist.std()\n",
    "        euclid_dist = (euclid_dist - mean_distance) / std_distance\n",
    "        euclid_dist = euclid_dist.sum().cpu().detach().numpy()\n",
    "\n",
    "        comb_pred_vectors = comb_pred_vectors.cpu().detach().numpy()\n",
    "        prob = proba_comb_features(gmm=best_gmm_model, comb_pred_vectors=comb_pred_vectors)\n",
    "        latentG_loss =  ( ALPHA * ( (1 - prob).sum())  +  BETA * (euclid_dist) )\n",
    "        latentG_loss = torch.from_numpy(np.array(latentG_loss)).to(DEVICE)\n",
    "        latentG_loss = classification_loss * (1 + (epoch / EPOCHS) * latentG_loss)  +  reconstruction_loss * 75\n",
    "        train_running_loss += latentG_loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        latentG_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss = train_running_loss / (idx+1)\n",
    "\n",
    "    student_model.eval()\n",
    "    test_running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (X,y) in enumerate(tqdm(test_dataloader)):\n",
    "            X = X.to(DEVICE).unsqueeze(1)\n",
    "            y = y.to(DEVICE).squeeze(1)\n",
    "            class_pred, reconstructed_pred, student_latent_vector_batch = student_model(X)\n",
    "            classification_loss = criterion1(class_pred, y)\n",
    "            reconstruction_loss = criterion2(reconstructed_pred, X)\n",
    "            total_test_loss = classification_loss + reconstruction_loss * 75\n",
    "            test_running_loss += total_test_loss.item()\n",
    "        test_loss = test_running_loss / (idx+1)\n",
    "    print(f\" EPOCH : {epoch + 1} | Train Loss : {train_loss:.4f} \")\n",
    "    print(f\" EPOCH : {epoch + 1} | Test Loss : {test_loss:.4f} \")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    if test_loss < best_loss :\n",
    "        best_model = deepcopy(student_model.state_dict())\n",
    "        best_model_results[\"train_loss\"] = train_loss\n",
    "        best_model_results[\"test_loss\"] = test_loss\n",
    "        best_loss = test_loss\n",
    "        \n",
    "    scheduler.step()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Saving the latest model...\")\n",
    "torch.save(student_model.state_dict(), MODEL_SAVE_PATH)\n",
    "torch.save(best_model, BEST_MODEL_SAVE_PATH) \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_generic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
